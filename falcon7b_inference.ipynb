{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2762a9a65c2a4c1a865ca7d133d9087d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9bb9901fdff641c6886864d213c9008f",
              "IPY_MODEL_8ffbf1b505104d87a24cf365c3360c24",
              "IPY_MODEL_8757e3535cbe465db9efcf3730032e10"
            ],
            "layout": "IPY_MODEL_897d7205ca6740e199d7895d075ffbf0"
          }
        },
        "9bb9901fdff641c6886864d213c9008f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48f5c25a33634d49a82c3f916fad5caf",
            "placeholder": "​",
            "style": "IPY_MODEL_8aa2342218d1418e8e842c51e0bdc593",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "8ffbf1b505104d87a24cf365c3360c24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc2bf039c9474234915da80c78c0b30a",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ae7f92cca3e4563b8eeec8cc9a0439d",
            "value": 2
          }
        },
        "8757e3535cbe465db9efcf3730032e10": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_63d2b067ebdd4a97b086a1c39180b6a2",
            "placeholder": "​",
            "style": "IPY_MODEL_f981a4d2f74a471fbf24d260fd2e233f",
            "value": " 2/2 [01:03&lt;00:00, 29.34s/it]"
          }
        },
        "897d7205ca6740e199d7895d075ffbf0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "48f5c25a33634d49a82c3f916fad5caf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8aa2342218d1418e8e842c51e0bdc593": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc2bf039c9474234915da80c78c0b30a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ae7f92cca3e4563b8eeec8cc9a0439d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "63d2b067ebdd4a97b086a1c39180b6a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f981a4d2f74a471fbf24d260fd2e233f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3cAVhCSKevvg",
        "outputId": "f2f8e51e-fb4c-4557-d139-29a7ba27e987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.28.0-py3-none-any.whl (290 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.1/290.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.20.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m42.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m41.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.99-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m55.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, accelerate\n",
            "Successfully installed accelerate-0.28.0 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.99 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch"
      ],
      "metadata": {
        "id": "zC5bXcCZfSkB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n"
      ],
      "metadata": {
        "id": "ywhW7TLJzwHR"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = \"tiiuae/falcon-7b-instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "pipeline = transformers.pipeline(\n",
        "    \"text-generation\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    trust_remote_code=True,\n",
        "    device_map=\"auto\",\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "2762a9a65c2a4c1a865ca7d133d9087d",
            "9bb9901fdff641c6886864d213c9008f",
            "8ffbf1b505104d87a24cf365c3360c24",
            "8757e3535cbe465db9efcf3730032e10",
            "897d7205ca6740e199d7895d075ffbf0",
            "48f5c25a33634d49a82c3f916fad5caf",
            "8aa2342218d1418e8e842c51e0bdc593",
            "cc2bf039c9474234915da80c78c0b30a",
            "4ae7f92cca3e4563b8eeec8cc9a0439d",
            "63d2b067ebdd4a97b086a1c39180b6a2",
            "f981a4d2f74a471fbf24d260fd2e233f"
          ]
        },
        "id": "UIrHyYYRj6Gn",
        "outputId": "81492746-b0f0-4875-a512-1a118733a068"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "WARNING:transformers_modules.tiiuae.falcon-7b-instruct.cf4b3c42ce2fdfe24f753f0f0d179202fea59c99.configuration_falcon:\n",
            "WARNING: You are currently loading Falcon using legacy code contained in the model repository. Falcon has now been fully ported into the Hugging Face transformers library. For the most up-to-date and high-performance version of the Falcon model code, please update to the latest version of transformers and then load the model without the trust_remote_code=True argument.\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2762a9a65c2a4c1a865ca7d133d9087d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sequences = pipeline(\n",
        "   \"The car had been in the long-term lot for about two weeks, said Greg Trevor, a spokesman for the Port Authority of New York and New Jersey. What happened since Greg Trevor said?\",\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "for seq in sequences:\n",
        "    print(f\"Result: {seq['generated_text']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rtGifmJjHjw",
        "outputId": "a01737f5-c932-4c75-e684-d2aacd408aac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: The car had been in the long-term lot for about two weeks, said Greg Trevor, a spokesman for the Port Authority of New York and New Jersey. What happened since Greg Trevor said?\n",
            "The Port Authority said in June that it was reviewing security and procedures for the long-term lots where some drivers were killed.\n",
            "The Port Authority Police Department has been working with the New Jersey State Police to investigate a possible shooting at a long-term parking lot in Jersey City on Tuesday.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prompt for a Q&A style\n",
        "question = \"What happened since Greg Trevor said, 'The car had been in the long-term lot for about two weeks'?\"\n",
        "\n",
        "sequences = pipeline(\n",
        "    question,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Assuming the model returns a single sequence as an answer\n",
        "for seq in sequences:\n",
        "    answer = seq['generated_text'].split(question)[-1]  # Extract the text after the question\n",
        "    print(f\"Answer: {answer.strip()}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsOL7oVAlO-o",
        "outputId": "bd5579b6-d961-4641-b673-6d11fe2e2850"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: I'm not sure what specific incident or conversation you're referring to. Can you provide more context or information?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how to format your prompt to encourage a Q&A style response\n",
        "question = \"What happened since Greg Trevor said, 'The car had been in the long-term lot for about two weeks'?\"\n",
        "prompt = f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "sequences = pipeline(\n",
        "    prompt,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Assuming the model returns a single sequence that includes both the question and answer\n",
        "for seq in sequences:\n",
        "    # Extract everything after 'Answer:'\n",
        "    answer = seq['generated_text'].split('Answer:')[-1].strip()\n",
        "    print(f\"Answer: {answer}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qANfbIXKloxC",
        "outputId": "ac8e67cc-bf11-47e0-b19b-4b45cc6ef46d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: Greg Trevor was referring to his son, Greg Jr. who passed away in a tragic accident on the side of the A-7 highway while playing basketball. Greg Jr. was driving his father's 1985 Pontiac Trans am when the car veered into a truck that was parked on the side of the highway. It is believed that Greg's death was a direct result of the head-on collision.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial conversational context\n",
        "conversation_context = \"\"\"Daniel: Hello, Girafatron! How's your day going?\n",
        "Girafatron: As always, I'm feeling amazing! Girafatron is always filled with positive vibes because he's always looking at the bright side of things!\n",
        "Daniel: That's wonderful to hear. What have you been up to lately?\n",
        "Girafatron:\"\"\"\n",
        "\n",
        "# Generate a continuation of the conversation\n",
        "sequences = pipeline(\n",
        "    conversation_context,\n",
        "    max_length=200,  # Adjust the length to your needs\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Print the entire conversation including the newly generated continuation\n",
        "for seq in sequences:\n",
        "    print(seq['generated_text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rrpL3Hmxmk-F",
        "outputId": "282552e8-4fa1-46f0-e7f2-d1751903b12a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Daniel: Hello, Girafatron! How's your day going?\n",
            "Girafatron: As always, I'm feeling amazing! Girafatron is always filled with positive vibes because he's always looking at the bright side of things!\n",
            "Daniel: That's wonderful to hear. What have you been up to lately?\n",
            "Girafatron: I've been keeping my schedule packed with fun activities. I'm always on the go, and I love it!\n",
            "Daniel: That's great to know. Speaking of activities, what would you like to do this weekend?\n",
            "Girafatron: There are so many options! Maybe we could go for a hike in the forest, take a trip to the beach, or just spend the day exploring the city.\n",
            "Daniel: Those are all great ideas. Whatever you choose, I'm sure it'll be an awesome time. Can't wait to hear about\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the conversation context as a string\n",
        "conversation_context = (\n",
        "    \"Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. \"\n",
        "    \"Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\\n\"\n",
        "    \"Daniel: Hello, Girafatron!\\n\"\n",
        "    \"Girafatron:\"\n",
        ")\n",
        "\n",
        "# Generate a continuation of the conversation using the pipeline\n",
        "sequences = pipeline(\n",
        "    conversation_context,\n",
        "    max_length=200,  # Adjust the length according to how much you want the model to generate\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Output the full conversation including the model's generated continuation\n",
        "for seq in sequences:\n",
        "    # Append the generated text to the initial context for a full conversation\n",
        "    full_conversation = conversation_context + seq['generated_text'][len(conversation_context):]\n",
        "    print(full_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EeHErs5QmsmL",
        "outputId": "3064cca8-c151-4000-94fa-f4deb277d6f1"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Girafatron is obsessed with giraffes, the most glorious animal on the face of this Earth. Giraftron believes all other animals are irrelevant when compared to the glorious majesty of the giraffe.\n",
            "Daniel: Hello, Girafatron!\n",
            "Girafatron: Hi! It's so nice to meet you finally!\n",
            "Daniel: Wow, you're a tall one!\n",
            "Girafatron: Thank you! I love my height. How about you, do you love animals?\n",
            "Daniel: Oh, of course!\n",
            "Girafatron: That's great to hear! So, do you have any favorite animals?\n",
            "Daniel: Yes, I love dogs and cats.\n",
            "Girafatron: That's wonderful! I am a huge fan of those animals too.\n",
            "Daniel: That's interesting! What do you think about the elephant and the zebra?\n",
            "Girafatron:\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation_context = (\n",
        "    \"The car had been in the long-term lot for about two weeks, said Greg Trevor, \"\n",
        "    \"a spokesman for the Port Authority of New York and New Jersey. \"\n",
        "    \"What happened since Greg Trevor said?\"\n",
        ")\n",
        "\n",
        "# Generate a continuation of the conversation using the pipeline\n",
        "sequences = pipeline(\n",
        "    conversation_context,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Output the full conversation including the model's generated continuation\n",
        "for seq in sequences:\n",
        "    # Append the generated text to the initial context for a full conversation\n",
        "    full_conversation = conversation_context + seq['generated_text'][len(conversation_context):]\n",
        "    print(full_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XdZsspXI9q6x",
        "outputId": "ff5553f8-b895-427c-fb8f-f016afbece7a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The car had been in the long-term lot for about two weeks, said Greg Trevor, a spokesman for the Port Authority of New York and New Jersey. What happened since Greg Trevor said?\n",
            "“I am sorry for any inconvenience this may have caused you or your loved ones. If you have any further questions or concerns, please do not hesitate to ask. The Port Authority Police, Port Authority staff and our contractor will do everything possible to keep you safe and your commute on track.\n",
            "“Thank you for your cooperation.”\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation_context = (\n",
        "    \"Greg Trevor: The car had been in the long-term lot for about two weeks.\\n\"\n",
        "    \"You: What happened since Greg Trevor said?\"\n",
        ")\n",
        "\n",
        "# Generate a continuation of the conversation using the pipeline\n",
        "sequences = pipeline(\n",
        "    conversation_context,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Output the full conversation including the model's generated continuation\n",
        "for seq in sequences:\n",
        "    # Append the generated text to the initial context for a full conversation\n",
        "    full_conversation = conversation_context + seq['generated_text'][len(conversation_context):]\n",
        "    print(full_conversation)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PjRV5bl6-Y0I",
        "outputId": "e14c7daa-0f25-487d-c418-6c2c60510e15"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greg Trevor: The car had been in the long-term lot for about two weeks.\n",
            "You: What happened since Greg Trevor said?\n",
            "We are in the middle of a pandemic. We need to take precautions to protect those in high risk categories and those around us.\n",
            "Greg Trevor: You need to have a mask in place. It needs to be on, and it needs to be covered up. If you are in a place where you cannot wear a mask, you need to make sure that your social distance and you're not in a position where you're going to be in contact of each other.\n",
            "You need to make sure that your social distancing is taking place, because the COVID virus is very contagious.\n",
            "Greg Trevor: They have been out there. They have been out of their homes for months at a time. They are getting tired, and they are getting bored. They want to get back out, but they need to protect themselves so that they can do that\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation_context = (\n",
        "    \"The coalition won in 2002 on a wave of euphoria after 24 years of rule by Daniel Arap Moi, \"\n",
        "    \"but now is in a precarious position because of growing public dissatisfaction. \"\n",
        "    \"Both camps agree that the existing constitution is outdated and oppressive \"\n",
        "    \"but have failed to reach consensus on the new one, which has been the subject of debate since 1997. \"\n",
        "    \"What event has already finished?\"\n",
        ")\n",
        "\n",
        "# Generate a continuation of the conversation using the pipeline\n",
        "sequences = pipeline(\n",
        "    conversation_context,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Define a function to format the response with line breaks\n",
        "def break_sentences(text):\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "# Assuming the response is a dictionary with a 'generated_text' key\n",
        "\n",
        "\n",
        "# Output the conversation including the model's response\n",
        "full_conversation = conversation_context + formatted_response[len(conversation_context):]\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nY_5Z8pn-6Kf",
        "outputId": "5a272cea-5dd0-425f-ba11-000acb1708a4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The coalition won in 2002 on a wave of euphoria after 24 years of rule by Daniel Arap Moi, but now is in a precarious position because of growing public dissatisfaction.\n",
            "Both camps agree that the existing constitution is outdated and oppressive but have failed to reach consensus on the new one, which has been the subject of debate since 1997.\n",
            "What event has already finished? embargo will remain in effect until Iraq pays war reparations to Kuwait to cover war damages.\n",
            "What will happen while something succeeds?\n",
            "If the allies succeed, Saddam Hussein will be deposed, Iraq would be under international sanctions, and Iraq's military infrastructure and its military forces would be destroyed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "for sample in data_samples:\n",
        "    # Define the conversation context as a string, and format it like a conversation\n",
        "    reporter_name = \"Reporter\"  # Assuming a reporter is speaking\n",
        "    you_name = \"You\"  # The person asking the question\n",
        "\n",
        "    conversation_context = f\"{reporter_name}: {sample['Context'].strip()} {you_name}: {sample['Question']}\"\n",
        "\n",
        "    # Generate a continuation of the conversation using the pipeline\n",
        "    sequences = pipeline(\n",
        "        conversation_context,\n",
        "        max_length=200,\n",
        "        do_sample=True,\n",
        "        top_k=10,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    # Output the full conversation including the model's generated continuation\n",
        "    for seq in sequences:\n",
        "        # Append the generated text to the initial context for a full conversation\n",
        "        full_conversation = conversation_context + seq['generated_text'][len(conversation_context):]\n",
        "        print(full_conversation)\n",
        "        print(\"\\n\" + \"-\"*50 + \"\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRNTjPdy_xbn",
        "outputId": "eca14d7f-27d9-45f3-b238-a17611f9b93a"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reporter: The car had been in the long-term lot for about two weeks, said Greg Trevor, a spokesman for the Port Authority of New York and New Jersey. You: What happened since Greg Trevor said?\n",
            "\"A man is dead after being shot to death in a 2006 Honda Civic in a lot outside of New York's Kennedy International Airport, according to a Port Authority spokesman. Port Authority officials say a man is dead after getting in a car accident outside a JFK long-term parking lot.\n",
            "New York Police have said the shooting is not random.\"\n",
            "This is so upsetting to see. My heart goes out to the family of this man. I'm so glad he got help from a stranger, but it's just sad and tragic this could have happened at such a place.\n",
            "This is such a sad story. I'm so glad to hear that he got help, but it's still horrible.\n",
            "This is terrible. What a horrible\n",
            "\n",
            "--------------------------------------------------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation_context = (\n",
        "    \"Cuban exiles in Miami will now 'proceed with all their resources to impede or delay' the ruling, \"\n",
        "    \"Cuba predicted Wednesday. \"\n",
        "    \"What event has already finished?\"\n",
        ")\n",
        "\n",
        "# Call the pipeline function to generate the response\n",
        "response = pipeline(\n",
        "    conversation_context,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Define a function to format the response with line breaks\n",
        "def break_sentences(text):\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "# Assuming the response is a dictionary with a 'generated_text' key\n",
        "formatted_response = break_sentences(response[0]['generated_text'])\n",
        "\n",
        "# Output the conversation including the model's response\n",
        "full_conversation = conversation_context + formatted_response[len(conversation_context):]\n",
        "print(full_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp3h0E47CCE5",
        "outputId": "4be7d4e9-6d8c-4b03-ac95-fec2bf43abcf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cuban exiles in Miami will now 'proceed with all their resources to impede or delay' the ruling, Cuba predicted Wednesday. What event has already finished?\n",
            "The event already finished is the Cuban exiles in Miami 'proceeding all their sources to obstruct' the ruling of a court in Cuba.\n",
            "The ruling in question is an attempt to revoke the 1962 Cuban Exile Act, which has been in place for nearly five decades.\n",
            "The ruling will now have to be reviewed and challenged by the Cuban government, and if successful, could be repealed.\n",
            "Cuban exiles have been working to repeal the act for years.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation_context = (\n",
        "    \"He also referred to the Daily Mail's support for facism and its opposition to allowing Jews \"\n",
        "    \"fleeing European pogroms to come to Britain in the 1930s. \"\n",
        "    \"'What Mr. Livingstone was doing when he made the statement which has led to this hearing is to \"\n",
        "    \"reflect his long-held and, it's accepted, honestly held political view of the Mail group,' \"\n",
        "    \"Child told a disciplinary hearing in central London. \"\n",
        "    \"What happened during the reference?\"\n",
        ")\n",
        "\n",
        "# Call the pipeline function to generate the response\n",
        "response = pipeline(\n",
        "    conversation_context,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Define a function to format the response with line breaks\n",
        "def break_sentences(text):\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "\n",
        "full_conversation = conversation_context + formatted_response[len(conversation_context):]\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WZmmr27CjFF",
        "outputId": "1bf614c9-57c5-46f9-fe87-801de1473a52"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "He also referred to the Daily Mail's support for facism and its opposition to allowing Jews fleeing European pogroms to come to Britain in the 1930s.\n",
            "'What Mr.\n",
            "Livingstone was doing when he made the statement which has led to this hearing is to reflect his long-held and, it's accepted, honestly held political view of the Mail group,' Child told a disciplinary hearing in central London.\n",
            "What happened during the reference?war reparations to Kuwait to cover war damages.\n",
            "What will happen while something succeeds?\n",
            "If the allies succeed, Saddam Hussein will be deposed, Iraq would be under international sanctions, and Iraq's military infrastructure and its military forces would be destroyed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "conversation_context = (\n",
        "    \"If the allies succeed, Saddam Hussein will have plunged his country first into a fruitless \"\n",
        "    \"eight-year war against Iran and then into a humiliating war against the U.S. and the allies \"\n",
        "    \"to defend his conquest of Kuwait, leaving much of his country's military establishment and \"\n",
        "    \"modern infrastructure in ruins. Meanwhile, the U.S. hopes, economic sanctions and an \"\n",
        "    \"international arms embargo will remain in effect until Iraq pays war reparations to Kuwait \"\n",
        "    \"to cover war damages. \"\n",
        "    \"What will happen while something succeeds?\"\n",
        ")\n",
        "\n",
        "# Call the pipeline function to generate the response\n",
        "response = pipeline(\n",
        "    conversation_context,\n",
        "    max_length=200,\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "# Define a function to format the response with line breaks\n",
        "def break_sentences(text):\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "full_conversation = conversation_context + formatted_response[len(conversation_context):]\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3G5FoMODC7nU",
        "outputId": "0eeba10b-66c3-4190-9e80-09c83633b621"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "If the allies succeed, Saddam Hussein will have plunged his country first into a fruitless eight-year war against Iran and then into a humiliating war against the U.S.\n",
            "and the allies to defend his conquest of Kuwait, leaving much of his country's military establishment and modern infrastructure in ruins.\n",
            "Meanwhile, the U.S.\n",
            "hopes, economic sanctions and an international arms embargo will remain in effect until Iraq pays war reparations to Kuwait to cover war damages.\n",
            "What will happen while something succeeds?\n",
            "If the allies succeed, Saddam Hussein will be deposed, Iraq would be under international sanctions, and Iraq's military infrastructure and its military forces would be destroyed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to break sentences\n",
        "def break_sentences(text):\n",
        "    # Replace each period followed by a space with a period and a newline\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "conversation_context = (\n",
        "    \"Canada's largest department store operator said the rights offering will entitle holders \"\n",
        "    \"of its ordinary shares, except residents in the U.S. and Britain, to subscribe for two additional \"\n",
        "    \"shares for every five shares held at a price of C$31.25 a share. The record date is Nov. 9.\"\n",
        ")\n",
        "question = \"What will happen in the future?\"\n",
        "\n",
        "# Generate a continuation of the conversation using the pipeline\n",
        "response = pipeline(\n",
        "    \"{}\\n{}\".format(conversation_context, question),\n",
        "    max_length=200,  # Adjust the length according to how much you want the model to generate\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "full_conversation = \"{}\\nAnswer: {}\".format(conversation_context, response[0]['generated_text'])\n",
        "\n",
        "# Break the conversation after each sentence\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvVmKxihErKl",
        "outputId": "a263dfb2-6497-4e2e-9e33-6137ea46cdfd"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Canada's largest department store operator said the rights offering will entitle holders of its ordinary shares, except residents in the U.S.\n",
            "and Britain, to subscribe for two additional shares for every five shares held at a price of C$31.25 a share.\n",
            "The record date is Nov.\n",
            "9.\n",
            "Answer: Canada's largest department store operator said the rights offering will entitle holders of its ordinary shares, except residents in the U.S.\n",
            "and Britain, to subscribe for two additional shares for every five shares held at a price of C$31.25 a share.\n",
            "The record date is Nov.\n",
            "9.\n",
            "What will happen in the future? The stock has been trading down for the last three-four days.\n",
            "Will it be a short term trade? Or a long term investment?\n",
            "As an investment, a lot depends on whether it is trading at a discount compared to other retailers or not.\n",
            "If it is trading at a discount and you can see a long term upturn, then it may make a good long term investment if it goes down in price.\n",
            "If it is trading at a discount, then it may be worth investing in this stock.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to break sentences\n",
        "def break_sentences(text):\n",
        "    # Replace each period followed by a space with a period and a newline\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "conversation_context = (\n",
        "    \"During the journey, Chinese and the U.S. navy held their first joint maritime exercise.\"\n",
        ")\n",
        "question = \"What happened during the joint maritime exercise?\"\n",
        "\n",
        "# Generate a continuation of the conversation using the pipeline\n",
        "response = pipeline(\n",
        "    \"{}\\n{}\".format(conversation_context, question),\n",
        "    max_length=200,  # Adjust the length according to how much you want the model to generate\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "full_conversation = \"{}\\nAnswer: {}\".format(conversation_context, response[0]['generated_text'])\n",
        "\n",
        "# Break the conversation after each sentence\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZS44fkBFNgi",
        "outputId": "6dff67b1-7bc2-4ed6-d7cf-bc9fa1f24e4a"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "During the journey, Chinese and the U.S.\n",
            "navy held their first joint maritime exercise.\n",
            "Answer: During the journey, Chinese and the U.S.\n",
            "navy held their first joint maritime exercise.\n",
            "What happened during the joint maritime exercise?\n",
            "Chinese Navy and the US Navy held a joint maritime exercise in the western Pacific on 5 May 2010.\n",
            "Chinese Navy's J-15 fighter jets took part in the exercise with the USS John C Stennis.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to break sentences into new lines\n",
        "def break_sentences(text):\n",
        "    # Replace each period followed by a space with a period and a newline\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "\n",
        "conversation_context = \"Li had said bluntly that it was 'the most unreasonable schedule' he had ever seen. 'Badminton competition demands physical strength,' said the head coach.\"\n",
        "\n",
        "# The question from the eighth data sample\n",
        "question = \"What event has already finished?\"\n",
        "\n",
        "# Generate a response using the model pipeline\n",
        "response = pipeline(\n",
        "    f\"{conversation_context}\\n{question}\",\n",
        "    max_length=200,  # Adjust as needed\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "\n",
        "full_conversation = f\"Context: {conversation_context}\\nQuestion: {question}\\nAnswer: {response[0]['generated_text']}\"\n",
        "\n",
        "# Format the conversation by breaking it into sentences\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IDykOOwFqoG",
        "outputId": "69a186f8-7894-49c1-ba94-63c3eb32ef0a"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: Li had said bluntly that it was 'the most unreasonable schedule' he had ever seen.\n",
            "'Badminton competition demands physical strength,' said the head coach.\n",
            "Question: What event has already finished?\n",
            "Answer: Li had said bluntly that it was 'the most unreasonable schedule' he had ever seen.\n",
            "'Badminton competition demands physical strength,' said the head coach.\n",
            "What event has already finished? The final, or the semi-finals?\n",
            "I'm afraid I don't know.\n",
            "What's your question?\n",
            "The badminton events are held before and after the opening and closing ceremonies, so it's difficult to tell if a specific competition has finished yet.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to break sentences into new lines\n",
        "def break_sentences(text):\n",
        "    # Replace each period followed by a space with a period and a newline\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "\n",
        "conversation_context = (\n",
        "    \"A few weeks after taking the reins of government from his brother on July 31, Raul Castro hinted \"\n",
        "    \"at some openness to dialogue with the United States in an interview with the Communist Party newspaper Granma, \"\n",
        "    \"suggesting ties might be normalized on equal terms. But the US had shot back that it was not talking to 'Fidel light.'\"\n",
        ")\n",
        "\n",
        "# The question from the ninth data sample\n",
        "question = \"What event has begun but has not finished?\"\n",
        "\n",
        "# Generate a response using the model pipeline\n",
        "response = pipeline(\n",
        "    f\"{conversation_context}\\n{question}\",\n",
        "    max_length=200,  # Adjust as needed\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "\n",
        "full_conversation = f\"Context: {conversation_context}\\nQuestion: {question}\\nAnswer: {response[0]['generated_text']}\"\n",
        "\n",
        "# Format the conversation by breaking it into sentences\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P4fS_M3QF4VD",
        "outputId": "58dc1a5e-f6be-4244-a94f-962335142c2f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: A few weeks after taking the reins of government from his brother on July 31, Raul Castro hinted at some openness to dialogue with the United States in an interview with the Communist Party newspaper Granma, suggesting ties might be normalized on equal terms.\n",
            "But the US had shot back that it was not talking to 'Fidel light.'\n",
            "Question: What event has begun but has not finished?\n",
            "Answer: A few weeks after taking the reins of government from his brother on July 31, Raul Castro hinted at some openness to dialogue with the United States in an interview with the Communist Party newspaper Granma, suggesting ties might be normalized on equal terms.\n",
            "But the US had shot back that it was not talking to 'Fidel light.'\n",
            "What event has begun but has not finished?\n",
            "It was 50 years ago that President John F.\n",
            "Kennedy was killed.\n",
            "(The anniversary was July 23) It took a month before Castro announced the death, and another month before the Soviet Union announced his death and released his body to public view.\n",
            "He was then buried in Santiago.\n",
            "The US was not consulted by Cuba.\n",
            "\"Fidel has been dead for a long time,\" the US ambassador told Raul Castro.\n",
            "What event has begun but is not yet finished?\n",
            "The Cuban government is celebrating the 30th anniversary of the Cuban Revolution on July 28.\n",
            "\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to break sentences into new lines\n",
        "def break_sentences(text):\n",
        "    # Replace each period followed by a space with a period and a newline\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "conversation_context = (\n",
        "    \"A respected figure and twice a member of the Chinese People's Political Consultative Congress, \"\n",
        "    \"Cai has used his scientific achievements to reach practical results for wetland conservation, \"\n",
        "    \"defending the idea that 'wetlands are closely linked to human survival and development.' \"\n",
        "    \"Cai also stressed the importance of wetlands for sustainable economic development and human well-being, \"\n",
        "    \"especially in their function as the 'kidneys of nature.'\"\n",
        ")\n",
        "\n",
        "# The question from the tenth data sample\n",
        "question = \"What happened before conservation?\"\n",
        "\n",
        "# Generate a response using the model pipeline\n",
        "response = pipeline(\n",
        "    f\"{conversation_context}\\n{question}\",\n",
        "    max_length=200,  # Adjust as needed\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "\n",
        "full_conversation = f\"Context: {conversation_context}\\nQuestion: {question}\\nAnswer: {response[0]['generated_text']}\"\n",
        "\n",
        "# Format the conversation by breaking it into sentences\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0A44vRKIF_n8",
        "outputId": "b8c1d872-5cfe-4d4f-90f1-2932efbc1966"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: A respected figure and twice a member of the Chinese People's Political Consultative Congress, Cai has used his scientific achievements to reach practical results for wetland conservation, defending the idea that 'wetlands are closely linked to human survival and development.' Cai also stressed the importance of wetlands for sustainable economic development and human well-being, especially in their function as the 'kidneys of nature.'\n",
            "Question: What happened before conservation?\n",
            "Answer: A respected figure and twice a member of the Chinese People's Political Consultative Congress, Cai has used his scientific achievements to reach practical results for wetland conservation, defending the idea that 'wetlands are closely linked to human survival and development.' Cai also stressed the importance of wetlands for sustainable economic development and human well-being, especially in their function as the 'kidneys of nature.'\n",
            "What happened before conservation?\n",
            "In the early stages before the 21st century, Cai had been deeply involved in the development and construction of wetlands, and his work has been a key factor in promoting China's ecological progress.\n",
            "In the 1950s, Cai started research into the conservation and restoration of wetlands and lakes, which was one of China's most important scientific achievements in wetland conservation.\n",
            "He is one of the most important people to be involved in the conservation of wetlands and lakes in China, and the Chinese government has also made great efforts in the protection\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the function to break sentences into new lines\n",
        "def break_sentences(text):\n",
        "    # Replace each period followed by a space with a period and a newline\n",
        "    return text.replace(\". \", \".\\n\")\n",
        "\n",
        "conversation_context = (\n",
        "    \"The combined effect of these changes is expected to save the company about $4 million in interest expenses next year, \"\n",
        "    \"or six cents a share. Shoney's said the latest restructuring affected bank indebtedness that was incurred to finance \"\n",
        "    \"$585 million of the company's $728 million recapitalization that took place in 1988.\"\n",
        ")\n",
        "\n",
        "# The question from the eleventh data sample\n",
        "question = \"What happened before restructuring?\"\n",
        "\n",
        "# Generate a response using the model pipeline\n",
        "response = pipeline(\n",
        "    f\"{conversation_context}\\n{question}\",\n",
        "    max_length=200,  # Adjust as needed\n",
        "    do_sample=True,\n",
        "    top_k=10,\n",
        "    num_return_sequences=1,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    truncation=True\n",
        ")\n",
        "\n",
        "\n",
        "full_conversation = f\"Context: {conversation_context}\\nQuestion: {question}\\nAnswer: {response[0]['generated_text']}\"\n",
        "\n",
        "# Format the conversation by breaking it into sentences\n",
        "formatted_conversation = break_sentences(full_conversation)\n",
        "\n",
        "# Print the formatted conversation\n",
        "print(formatted_conversation)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkd3svzPGQOT",
        "outputId": "b1d627cb-ddf0-4d45-ca5c-5db0fc5833b3"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:11 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context: The combined effect of these changes is expected to save the company about $4 million in interest expenses next year, or six cents a share.\n",
            "Shoney's said the latest restructuring affected bank indebtedness that was incurred to finance $585 million of the company's $728 million recapitalization that took place in 1988.\n",
            "Question: What happened before restructuring?\n",
            "Answer: The combined effect of these changes is expected to save the company about $4 million in interest expenses next year, or six cents a share.\n",
            "Shoney's said the latest restructuring affected bank indebtedness that was incurred to finance $585 million of the company's $728 million recapitalization that took place in 1988.\n",
            "What happened before restructuring? The company had been unable to repay a $585 million bank loan, and had been forced to undergo a reorganization to prevent default on the debt.\n",
            "As part of that, Shoney closed 27 of its 42 outlets.\n",
            "Shoney's has a $4 million line of credit from its bank, which has not been used since the bankruptcy filing.\n",
            "The company had hoped to have the line increased after its current restructuring plan.\n",
            "Shoney's said the company's new financial structure should allow for the company's \"re-emergence in the future.\"\n",
            "The company\n"
          ]
        }
      ]
    }
  ]
}